---
title: "Simultaneous Autoregressive Model"
author: "Sam Song"
output: html_document
editor_options: 
  chunk_output_type: console
---

Simultaneous Autoregressive Model

$$Y = \beta_0 + \beta_1X + \rho\sum w_i(Y_i-\beta_0 - \beta_1X_i)$$
$$Y = \beta_0 + \beta_1X + \rho\sum w_iY_i$$

$\rho$ describes the degree of correlation with neighbors; if $\rho$ value is close to 1, it weights heavily and if $\rho$ value is close to 0, not much weight
$w_i$ is the weight on neighbor $i$. 

$Y_i-\beta_0 - \beta_1X_i$ is the residual!!

```{r, message = FALSE}
library(dplyr)
library(stringr)
library(tidyverse)
library(sf)
library(tmap)
library(spdep)
library(spatialreg)
```

## Data setup
```{r}
# read dataset (source: https://geodacenter.github.io/data-and-lab//comarea_vars/)
chicago_sf <- st_read("~/Documents/Data/ComArea/ComArea_ACS14_f.shp")

# read another dataset to add other variables (source: https://www.cmap.illinois.gov/data/data-hub)
chicago_census <- read_csv("~/Documents/Data/CMAP_2022/cds_202207/ReferenceCCAProfiles20162020.csv")

chisub <- chi15 %>% 
  # pick out necessary variables only
  select(GEOID, GEOG, `2000_POP`, `2010_POP`, MED_HA) %>%
  rename(Pop2000 = `2000_POP`,
         Pop2010 = `2010_POP`) %>%
  # create a column that shows the change in population between 2010 and 2000
  mutate(PopChange = (Pop2010-Pop2000)/Pop2000)

# join the selected variables above onto the first dataset by community ID
chicago_sf <- chicago_sf %>% left_join(chisub, by = c("ComAreaID"= "GEOID"))

# read the third dataset about the grocery store in Chicago (source: https://data.cityofchicago.org/Health-Human-Services/Grocery-Store-Status-Map/rish-pa6g)
grocery_store <- read_csv("~/Documents/Data/grocery_chicago.csv")

grocery_store <- grocery_store %>%
  # drop rows with missing geometry information
  filter(is.na(Location) == FALSE) %>%
  # extract latitude and longitude from the string
  mutate(x = str_split(Location, " ", simplify = TRUE)[,2],
         y = str_split(Location, " ", simplify = TRUE)[,3],
         # convert the extracted value to numeric
         x = as.numeric(str_replace_all(x, "\\(", "")),
         y = as.numeric(str_replace_all(y, "\\)", ""))) %>%
  select(- Location, - `Last updated`) %>%
  rename(status = `New status`,
         Chain = `Store Name`) %>%
  # filter out the online-only store as there is only one value
  filter(status != 'ONLINE ORDERS ONLY') %>%
  # transform the dataset to a sf object
  st_as_sf(coords = c("x", "y")) %>%
  # assign the Coordinate Reference System (WGS 84)
  st_set_crs(4236)

# transorm the Coordinate Reference System to match that of the first dataset.
grocery_store <- st_transform(grocery_store, st_crs(chicago_sf))

# find grocery stores within each neighborhood
grocery_nb <- st_join(grocery_store, chicago_sf, join = st_within) %>%
  filter(is.na(ComAreaID) == FALSE)

# count number of grocery stores
grocery_nb_cnt <- as_tibble(grocery_nb) %>%
  count(ComAreaID)

# join the grocery counts onto the original dataset
chicago_sf <- left_join(chicago_sf, grocery_nb_cnt) %>%
  rename(num_grocery = n) 

# finalize data preparation
chicago <- chicago_sf %>%
  # make sure there is no NAs by turning missing values to 0
  mutate(num_grocery = ifelse(is.na(num_grocery), 0 , num_grocery),
         # create a column that shows the number of grocery stores per 100,000 residents
         grocery_100k = num_grocery/Pop2014*100000)

         #log_grocery_100k = log((n_grocery + 1)/Pop2014*100000))
```

- Data tidying process

```{r}
# refined dataset
chicago
```

```{r}
# create a visualization to see if all of the neighborhoods of Chicago are included in the dataset
ggplot(chicago) +
  geom_sf(color = "grey", fill = "lightblue") +
  theme_bw() +
  labs(title = "Neighborhoods Map of Chicago")

# overlay the locations of grecery store
ggplot(chicago) +
  geom_sf(color = "grey", fill = "lightblue") +
  # grocery store locations in points
  geom_sf(data = grocery_store, size = 1, aes(color = status)) +
  theme_bw() +
  labs(title = "Grocery stores in Chicago")
```


2. What type of geometry does chicago_sup have? Would we consider this area or point pattern data?
- point

# Visualize
```{r}
ggplot(chicago_sf) +
   geom_sf()
```

```{r}
# left one is point pattern, and the right one is areal, at this point can't really directly compare the two
ggplot(chicago_sf) + 
  geom_sf() + 
  scale_fill_fermenter() + 
  geom_sf(data = grocery_store) + 
  theme_bw() + 
  labs(fill = "Per Capita\nIncome, 2014", 
       title = "Locations of grocery stores, 2015")
```


```{r}
# converted the point pattern data to areal data so that the comparison can be done 
ggplot(chicago) + 
  geom_sf(aes(fill = grocery_100k)) + 
  scale_fill_fermenter(palette = 2, direction = 1) + 
  labs(fill = "Grocery stores\nper 100k people") + 
  theme_bw()
```


# Moran's I review

3. Comparing the plots of Per capita income and Grocery stores per 100k, which variable do you think has stronger spatial autocorrelation? 

Whether we are looking at per capita income or at number of grocery stores, we start by creating the neighbors (nb) and the neighbor weights (nbw).

```{r}
# Create neigbors
chicago_nb <- poly2nb(chicago_sf, queen = TRUE)
# Create neighbor weights
chicago_nbw <- nb2listw(chicago_nb, style = "W", zero.policy = TRUE)
```

4. What does the code below do? Interpret the result.
```{r}
moran.mc(chicago$PerCInc14, chicago_nbw, nsim = 499)
```

There is a moderately strong spatial autocorrelation (I = .51) 

```{r}
moran.mc(chicago$Wht14P, chicago_nbw, nsim = 499)
```

There is a strong spatial autocorrelation (I = .69) in the percentage of residents of a neighborhood that identify as White.
Neighborhoods tend to have similar percentage of white residents as their neighbors.

5. Repeat #4 using the grocery_100k variable. Interpret the result.

```{r}
moran.mc(chicago$grocery_100k, chicago_nbw, nsim = 499)
```

There is a weaker spatial autocorrelation (I = .13) in the 


6. Comparing Per capita income and Grocery stores per 100k in #4 and #5, which variable do you think has stronger spatial autocorrelation? 



# Regression
Start by exploring relationships with other variables:
```{r}
ggplot(chicago) + 
  geom_point(aes(PerCInc14, grocery_100k))

ggplot(chicago) + 
  geom_point(aes(Wht14P, grocery_100k))  

ggplot(chicago) + 
  geom_point(aes(Wht14P, log(grocery_100k))) 


ggplot(chicago) + 
  geom_sf(aes(fill = Wht14P), color = "white") + 
  scale_fill_fermenter() + 
  geom_sf(data = grocery_store) + 
  theme_bw()
```

Start by fitting a linear regression model: 
```{r}
lm1 <- lm(log(grocery_100k + .1) ~ Wht14P, data= chicago_sf)
plot(lm1, 1)
summary(lm1)
```

Join residuals to sf and Plot Residuals:
```{r}
chicago_sf$resid1 <- residuals(lm1)
```


7. Using ggplot, make a chloropleth map of the residuals. 

```{r}
# looking at the colors of graph, the assumption of independent is violated, so ordinary linear regression method cannot be performed in here
ggplot(chicago_sf) +
  geom_sf(aes(fill = resid1), color = "white") + 
  scale_fill_gradient2()
```



8. Check moran's I:
```{r}
moran(chicago_sf$resid1, 
                       chicago_nbw, 
                       n = length(chicago_nb), 
                       S0 = Szero(chicago_nbw))
```

Fit spatial regression:
```{r}
sarlm1 <- lagsarlm(log_grocery_100k + ~ Wht14P, data = chicago_sf, listw = chicago_nbw)

summary(sarlm1)
```

- $\rho=$ 0.06; spatial autocorrelation in the number of grocery stores in neighboring communities is pretty low 
- if $\rho$ is small, OLS is a good model. if $\rho$ is big, OLS is not to be trusted. 

9. Compare the SAR (lagsarlm) and OLS (lm) models. Look at estimates of slope and intercept, the standard error, and p-value.


10. Write 2-3 concluding sentences about what you learned of the distribution of grocery stores throughout Chicago. Consider including ideas from your background readings.

- we caanot conclude satistical significance that regions with white people are correlated with more grocery store. Yet, aggregation and confounding variables exists in our analysis. It is also important to note that the graphs paint a different story than our original statistical analysis.

_ **Interpretation**: For every \% point increase in white residents, number of groceries per 100,000 residents is predicted to increase 1.002 times (or by .2\%).


If time: Try regression with other variables!

```{r}
ggplot(data = chicago_sf, aes(x = Pov14, y = TeenBirth)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

lm2 <- lm(Pov14 ~ TeenBirth, data = chicago_sf)
plot(lm2, 1)
summary(lm2)
```

```{r}
chicago_sf$resid2 <- residuals(lm2)

ggplot(chicago_sf) +
  geom_sf(aes(fill = resid2), color = "white") + 
  scale_fill_gradient2()

moran(chicago_sf$resid2, 
                       chicago_nbw, 
                       n = length(chicago_nb), 
                       S0 = Szero(chicago_nbw))

sarlm2 <- lagsarlm(Pov14 ~ TeenBirth, data = chicago_sf, listw = chicago_nbw)

summary(sarlm2)
```

```{r}
lm3 <- lm(log(PropCrRt) ~ PerCInc14 + Unemp14 + HSGrad14, data = chicago_sf)
plot(lm3, 1)
summary(lm3)

sarlm3 <- lagsarlm(log(PropCrRt) ~ PerCInc14 + Unemp14 + HSGrad14, data = chicago_sf, listw = chicago_nbw)
summary(sarlm3)
```

